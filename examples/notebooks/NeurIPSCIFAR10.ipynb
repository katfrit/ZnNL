{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f88cbc36",
   "metadata": {},
   "source": [
    "# NeurIPS CIFAR10 Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59807e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am disabling the GPU here, feel free to comment these lines out if your\n",
    "# Jax installation runs fine on your GPU.\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "\n",
    "import znrnd as rnd\n",
    "\n",
    "from neural_tangents import stax\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import numpy as np\n",
    "import optax\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "913e12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_generator = rnd.data.CIFAR10Generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d51cadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = stax.serial(\n",
    "    stax.Conv(32, (3, 3)),\n",
    "    stax.Relu(),\n",
    "    stax.AvgPool(window_shape=(2, 2), strides=(2, 2)),\n",
    "    stax.Conv(64, (3, 3)),\n",
    "    stax.Relu(),\n",
    "    stax.AvgPool(window_shape=(2, 2), strides=(2, 2)),\n",
    "    stax.Flatten(),\n",
    "    stax.Dense(256)\n",
    ")\n",
    "model1 = stax.serial(\n",
    "    stax.Conv(32, (3, 3)),\n",
    "    stax.Relu(),\n",
    "    stax.AvgPool((2, 2), (2, 2)),\n",
    "    stax.Conv(64, (3, 3)),\n",
    "    stax.Relu(),\n",
    "    stax.AvgPool((2, 2), (2, 2)),\n",
    "    stax.Flatten(),\n",
    "    stax.Dense(256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce33b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = rnd.models.NTModel(\n",
    "        nt_module=model,\n",
    "        optimizer=optax.sgd(0.001),\n",
    "        loss_fn=znrnd.loss_functions.MeanPowerLoss(order=2),\n",
    "        input_shape=(1, 32, 32, 3),\n",
    "        training_threshold=0.001\n",
    "    )\n",
    "\n",
    "predictor = rnd.models.NTModel(\n",
    "        nt_module=model1,\n",
    "        optimizer=optax.sgd(0.001),\n",
    "        loss_fn=znrnd.loss_functions.MeanPowerLoss(order=2),\n",
    "        input_shape=(1, 32, 32, 3),\n",
    "        training_threshold=0.001\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5fa542b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = rnd.agents.RND(\n",
    "        point_selector=znrnd.point_selection.GreedySelection(threshold=0.01),\n",
    "        distance_metric=znrnd.distance_metrics.OrderNDifference(order=2),\n",
    "        data_generator=data_generator,\n",
    "        target_network=target,\n",
    "        predictor_network=predictor,\n",
    "        tolerance=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7058b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(data_set_size: int, ensembling: bool = False, ensembles: int = 10):\n",
    "    \"\"\"\n",
    "    Run an experiment for a specific datasrndsize.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_set_size : int\n",
    "            Size of the dataset to produce\n",
    "    ensembling : bool (default=False)\n",
    "            If true, the experiment is run several times to produce an error estimate\n",
    "    ensembles : int\n",
    "            Number of ensembles to use in the averaging.\n",
    "            \n",
    "    Returns\n",
    "    -------\n",
    "    entropy : dict\n",
    "            A dictionary of the computed entropy:\n",
    "            e.g {\"rnd\": 0.68, \"random\": 0.41, \"approximate_maximum\": 0.84}\n",
    "    eigenvalues : dict\n",
    "            Dictionary of eigenvalues\n",
    "            e.g {\"rnd\": np.array(), \"random\": np.array(), \"approximate_maximum\": np.array()}\n",
    "\n",
    "    \"\"\"\n",
    "    # Turnoff averaging if required.\n",
    "    if not ensembling:\n",
    "        ensembles = 1\n",
    "    \n",
    "    rnd_entropy_arr = []\n",
    "    random_entropy_arr = []\n",
    "    apr_max_entropy_arr = []\n",
    "    \n",
    "    rnd_eig_arr = []\n",
    "    random_eig_arr = []\n",
    "    apr_max_eig_arr = []\n",
    "    \n",
    "    rnd_losses = []\n",
    "    random_losses = []\n",
    "    apr_max_losses = []\n",
    "    \n",
    "    for i in range(ensembles):\n",
    "    \n",
    "        # Define the models\n",
    "        target = rnd.models.NTModel(\n",
    "            nt_module=model,\n",
    "            optimizer=optax.sgd(0.001),\n",
    "            loss_fn=rnd.loss_functions.MeanPowerLoss(order=2),\n",
    "            input_shape=(1, 32, 32, 3),\n",
    "            training_threshold=0.001\n",
    "        )\n",
    "\n",
    "        predictor = rnd.models.NTModel(\n",
    "            nt_module=model1,\n",
    "            optimizer=optax.sgd(0.001),\n",
    "            loss_fn=rnd.loss_functions.MeanPowerLoss(order=2),\n",
    "            input_shape=(1, 32, 32, 3),\n",
    "            training_threshold=0.001\n",
    "        )\n",
    "\n",
    "        # Define the agents for a fresh run.\n",
    "        rnd_agent = rnd.agents.RND(\n",
    "            point_selector=rnd.point_selection.GreedySelection(threshold=0.01),\n",
    "            distance_metric=rnd.distance_metrics.OrderNDifference(order=2),\n",
    "            data_generator=data_generator,\n",
    "            target_network=target,\n",
    "            predictor_network=predictor,\n",
    "            tolerance=15\n",
    "        )\n",
    "        rnd_agent.target_set = []\n",
    "        rnd_agent.target_indices = []\n",
    "        \n",
    "        random_agent = rnd.agents.RandomAgent(data_generator=data_generator)\n",
    "        approximate_max_agent = rnd.agents.ApproximateMaximumEntropy(\n",
    "            target_network=target, \n",
    "            data_generator=data_generator,\n",
    "            samples=10,  # How many sets it produces in the test. Takes the one with max entropy.\n",
    "        )\n",
    "\n",
    "        # Compute the sets\n",
    "        rnd_set = rnd_agent.build_dataset(target_size=data_set_size, visualize=False)\n",
    "        random_set = random_agent.build_dataset(target_size=data_set_size, visualize=False)    \n",
    "        apr_max_set = approximate_max_agent.build_dataset(\n",
    "            target_size=data_set_size, visualize=False\n",
    "        )\n",
    "\n",
    "        # Compute NTK for each set\n",
    "        rnd_ntk = target.compute_ntk(x_i=rnd_set)[\"empirical\"]\n",
    "        random_ntk = target.compute_ntk(x_i=random_set)[\"empirical\"]\n",
    "        apr_max_ntk = target.compute_ntk(x_i=apr_max_set)[\"empirical\"]\n",
    "\n",
    "\n",
    "        # Compute the entropy of each set\n",
    "        rnd_entropy = rnd.analysis.EntropyAnalysis(matrix=rnd_ntk).compute_von_neumann_entropy()\n",
    "        random_entropy = rnd.analysis.EntropyAnalysis(matrix=random_ntk).compute_von_neumann_entropy()\n",
    "        apr_max_entropy = rnd.analysis.EntropyAnalysis(matrix=apr_max_ntk).compute_von_neumann_entropy()\n",
    "\n",
    "\n",
    "        # Compute eigenvalues\n",
    "        rnd_eigval = rnd.analysis.EigenSpaceAnalysis(matrix=rnd_ntk).compute_eigenvalues()\n",
    "        random_eigval = rnd.analysis.EigenSpaceAnalysis(matrix=random_ntk).compute_eigenvalues()\n",
    "        apr_max_eigval = rnd.analysis.EigenSpaceAnalysis(matrix=rnd_ntk).compute_eigenvalues()\n",
    "        \n",
    "        rnd_entropy_arr.append(rnd_entropy)\n",
    "        random_entropy_arr.append(random_entropy)\n",
    "        apr_max_entropy_arr.append(apr_max_entropy)\n",
    "        \n",
    "        rnd_eig_arr.append(rnd_eigval)\n",
    "        random_eig_arr.append(random_eigval)\n",
    "        apr_max_eig_arr.append(apr_max_eigval)\n",
    "        \n",
    "        # Train production model\n",
    "        rnd_production = rnd.models.FlaxModel(\n",
    "            flax_module=CustomModule(),\n",
    "            optimizer=optax.adam(learning_rate=0.001),\n",
    "            loss_fn=rnd.loss_functions.CrossEntropyLoss(classes=10),\n",
    "            input_shape=(1, 32, 32, 3),\n",
    "            training_threshold=0.001\n",
    "        )\n",
    "        \n",
    "        random_production = rnd.models.FlaxModel(\n",
    "            flax_module=CustomModule(),\n",
    "            optimizer=optax.adam(learning_rate=0.001),\n",
    "            loss_fn=rnd.loss_functions.CrossEntropyLoss(classes=10),\n",
    "            input_shape=(1, 32, 32, 3),\n",
    "            training_threshold=0.001\n",
    "        )\n",
    "        \n",
    "        apr_max_production = rnd.models.FlaxModel(\n",
    "            flax_module=CustomModule(),\n",
    "            optimizer=optax.adam(learning_rate=0.001),\n",
    "            loss_fn=rnd.loss_functions.CrossEntropyLoss(classes=10),\n",
    "            input_shape=(1, 32, 32, 3),\n",
    "            training_threshold=0.001\n",
    "        )\n",
    "        \n",
    "        \n",
    "        rnd_training_ds = {\n",
    "            \"inputs\": np.take(data_generator.ds_train[\"image\"], rnd_agent.target_indices, axis=0),\n",
    "            \"targets\": np.take(data_generator.ds_train[\"label\"], rnd_agent.target_indices, axis=0)\n",
    "        }\n",
    "        random_training_ds = {\n",
    "            \"inputs\": np.take(data_generator.ds_train[\"image\"], random_agent.target_indices, axis=0),\n",
    "            \"targets\": np.take(data_generator.ds_train[\"label\"], random_agent.target_indices, axis=0)\n",
    "        }\n",
    "        apr_max_training_ds = {\n",
    "            \"inputs\": np.take(data_generator.ds_train[\"image\"], approximate_max_agent.target_indices, axis=0),\n",
    "            \"targets\": np.take(data_generator.ds_train[\"label\"], approximate_max_agent.target_indices, axis=0)\n",
    "        }\n",
    "        \n",
    "        test_ds = {\n",
    "            \"inputs\": data_generator.ds_test[\"image\"],\n",
    "            \"targets\": data_generator.ds_test[\"label\"]\n",
    "        }\n",
    "        \n",
    "        rnd_losses.append(\n",
    "            rnd_production.train_model(train_ds=rnd_training_ds, test_ds=test_ds)\n",
    "        )\n",
    "        random_losses.append(\n",
    "            random_production.train_model(train_ds=random_training_ds, test_ds=test_ds)\n",
    "        )\n",
    "        apr_max_losses.append(apr_max_production.train_model(train_ds=apr_max_training_ds, test_ds=test_ds))\n",
    "        \n",
    "        \n",
    "        del rnd_agent\n",
    "        del random_agent\n",
    "        del approximate_max_agent\n",
    "    \n",
    "    \n",
    "    # Get mean and uncertainty.\n",
    "    rnd_entropy_arr = np.array(rnd_entropy_arr)\n",
    "    random_entropy_arr = np.array(random_entropy_arr)\n",
    "    apr_max_entropy_arr = np.array(apr_max_entropy_arr)\n",
    "    \n",
    "    rnd_eig_arr = np.array(rnd_eig_arr)\n",
    "    random_eig_arr = np.array(random_eig_arr)\n",
    "    apr_max_eig_arr = np.array(apr_max_eig_arr)\n",
    "    \n",
    "    rnd_losses = np.array(rnd_losses)\n",
    "    random_losses = np.array(random_losses)\n",
    "    apr_max_losses = np.array(apr_max_losses)\n",
    "    \n",
    "    rnd_entropy = np.array(\n",
    "        [np.mean(rnd_entropy_arr), np.std(rnd_entropy_arr) / np.sqrt(ensembles)]\n",
    "    )\n",
    "    random_entropy = np.array(\n",
    "        [np.mean(random_entropy_arr), np.std(random_entropy_arr) / np.sqrt(ensembles)]\n",
    "    )\n",
    "    apr_max_entropy = np.array(\n",
    "        [np.mean(apr_max_entropy_arr), np.std(apr_max_entropy_arr) / np.sqrt(ensembles)]\n",
    "    )\n",
    "\n",
    "    rnd_eigval = np.array(\n",
    "        [np.mean(rnd_eig_arr, axis=0), np.std(rnd_eig_arr, axis=0) / np.sqrt(ensembles)]\n",
    "    )\n",
    "    random_eigval = np.array(\n",
    "        [np.mean(random_eig_arr, axis=0), np.std(random_eig_arr, axis=0) / np.sqrt(ensembles)]\n",
    "    )\n",
    "    apr_max_eigval = np.array(\n",
    "        [np.mean(apr_max_eig_arr, axis=0), np.std(apr_max_eig_arr, axis=0) / np.sqrt(ensembles)]\n",
    "    )\n",
    "    \n",
    "    rnd_loss = np.array(\n",
    "        [np.mean(rnd_losses, axis=0), np.std(rnd_losses, axis=0) / np.sqrt(ensembles)]\n",
    "    )\n",
    "    random_loss = np.array(\n",
    "        [np.mean(random_losses, axis=0), np.std(random_losses, axis=0) / np.sqrt(ensembles)]\n",
    "    )\n",
    "    apr_max_loss = np.array(\n",
    "        [np.mean(apr_max_losses, axis=0), np.std(apr_max_losses, axis=0) / np.sqrt(ensembles)]\n",
    "    )\n",
    "    \n",
    "    entropy = {\"rnd\": rnd_entropy, \"random\": random_entropy, \"approximate_maximum\": apr_max_entropy}\n",
    "    eigenvalues = {\"rnd\": rnd_eigval, \"random\": random_eigval, \"approximate_maximum\": apr_max_eigval}\n",
    "    losses = {\"rnd\": rnd_loss, \"random\": random_loss, \"approximate_maximum\": apr_max_loss}\n",
    "    \n",
    "    return entropy, eigenvalues, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf68346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100: 100%|████████████████████████████████| 100/100 [00:03<00:00, 29.18batch/s, test_loss=60]\n",
      "Epoch: 110: 100%|████████████████████████████| 110/110 [00:03<00:00, 29.47batch/s, test_loss=0.0248]\n",
      "Epoch: 121: 100%|███████████████████████████| 121/121 [00:04<00:00, 28.89batch/s, test_loss=5.35e-6]\n",
      "/tikhome/stovey/miniconda3/envs/zincware/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning:\n",
      "\n",
      "The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "\n",
      "/tikhome/stovey/miniconda3/envs/zincware/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning:\n",
      "\n",
      "The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "\n",
      "/tikhome/stovey/miniconda3/envs/zincware/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning:\n",
      "\n",
      "The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "\n",
      "Epoch: 100: 100%|██████████████████████████████| 100/100 [00:06<00:00, 15.51batch/s, test_loss=94.7]\n",
      "Epoch: 110: 100%|██████████████████████████████| 110/110 [00:07<00:00, 14.79batch/s, test_loss=6.16]\n",
      "Epoch: 121: 100%|█████████████████████████████| 121/121 [00:07<00:00, 15.44batch/s, test_loss=0.298]\n",
      "Epoch: 133: 100%|████████████████████████████| 133/133 [00:08<00:00, 15.25batch/s, test_loss=0.0116]\n",
      "Epoch: 146: 100%|██████████████████████████| 146/146 [00:09<00:00, 15.19batch/s, test_loss=0.000346]\n",
      "/tikhome/stovey/miniconda3/envs/zincware/lib/python3.9/site-packages/sklearn/manifold/_t_sne.py:982: FutureWarning:\n",
      "\n",
      "The PCA initialization in TSNE will change to have the standard deviation of PC1 equal to 1e-4 in 1.2. This will ensure better convergence.\n",
      "\n",
      "Epoch: 100: 100%|██████████████████████████████| 100/100 [00:09<00:00, 10.89batch/s, test_loss=90.6]\n",
      "Epoch: 110: 100%|██████████████████████████████| 110/110 [00:10<00:00, 10.71batch/s, test_loss=12.2]\n",
      "Epoch: 121: 100%|██████████████████████████████| 121/121 [00:11<00:00, 10.69batch/s, test_loss=1.45]\n",
      "Epoch: 133: 100%|█████████████████████████████| 133/133 [00:12<00:00, 10.76batch/s, test_loss=0.149]\n",
      "Epoch: 146: 100%|█████████████████████████████| 146/146 [00:13<00:00, 10.65batch/s, test_loss=0.013]\n",
      "Epoch: 159:  99%|█████████████████████████▋| 158/160 [00:15<00:00, 10.10batch/s, test_loss=0.000983]"
     ]
    }
   ],
   "source": [
    "run_experiment(3, ensembling=True, ensembles=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045d3ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
